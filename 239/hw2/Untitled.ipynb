{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import string\n",
    "\n",
    "#read file\n",
    "def readFile(path):\n",
    "    with open(path,\"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        docs_raw = [l.split() for l in lines]\n",
    "        res = [int(docs_raw[i][0])for i in range(len(docs_raw))]\n",
    "    return docs_raw, res\n",
    "\n",
    "def readTest(path):\n",
    "    with open(path,\"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        docs_raw = [l.split() for l in lines]\n",
    "        #res = [int(docs_raw[i][0])for i in range(len(docs_raw))]\n",
    "    return docs_raw\n",
    "\n",
    "\n",
    "#filter doc length\n",
    "def filterLen(docs, minlen):\n",
    "    docs_raw4 = [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "    s = \" \"\n",
    "    #docs = [s.join(d) for d in docs_raw4]\n",
    "    docs = [(s.join(d)).translate(None, string.punctuation) for d in docs_raw4]\n",
    "    return docs\n",
    "\n",
    "# Train on a 80/20 split\n",
    "def splitSet(docs,res , testSize, stateNum):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test,y_train,y_test = train_test_split(docs,res, test_size=testSize, random_state =stateNum)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#make a lemmatizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "#convert into a matrix of token counts\n",
    "def makeVectorizer(idf,ngram, max_df,min_df,norm,stopwords,lemmatizer):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "    \n",
    "    if idf:\n",
    "        vectorizer = TfidfVectorizer(lowercase = True,\n",
    "                                    stop_words = stopwords,\n",
    "                                     use_idf = idf,\n",
    "                                     max_df = max_df,\n",
    "                                     min_df = min_df,\n",
    "                                     ngram_range = ngram,\n",
    "                                     norm = norm,\n",
    "                                     tokenizer = lemmatizer)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(lowercase = True,\n",
    "                                    stop_words = stopwords,\n",
    "                                     max_df = max_df,\n",
    "                                     min_df = min_df,\n",
    "                                    ngram_range = ngram)\n",
    "                                    \n",
    "    return vectorizer\n",
    "\n",
    "#l2 norm\n",
    "def l2Norm(data):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    normalizerl2 = Normalizer(norm = \"l2\")\n",
    "    normalizerl2.fit(data)\n",
    "    data_l2 = normalizerl2.transform(data)\n",
    "    return data_l2\n",
    "\n",
    "\n",
    "# compute the similary of the test set data to all the data in the training set\n",
    "# save the pairwise similary into a similarity matrix for later knn use\n",
    "def cosSim(test, train):\n",
    "    from sklearn.metrics.pairwise import linear_kernel\n",
    "    cosine_similarities = linear_kernel(test, train)\n",
    "    return cosine_similarities\n",
    "\n",
    "#sort the top k sim \n",
    "def sortInd(cosSim,k):\n",
    "    sortedInd = np.argpartition(cosSim, -k)[:,-k:]\n",
    "    return sortedInd\n",
    "\n",
    "#knn\n",
    "def knnDis(topInd_mat, cosSim_mat,threshold):\n",
    "    rows = topInd_mat.shape[0]\n",
    "    y_test_dis =[]\n",
    "    res_sim = []\n",
    "    for i in range(rows):\n",
    "        topInd_vec = topInd_mat[i]\n",
    "        res =sum(int(y_train[j]) * cosSim_mat[i][j] for j in topInd_vec)/sum([cosSim_mat[i][j] for j in topInd_vec])\n",
    "        \n",
    "        res_sim.append(res)\n",
    "        if res >=threshold:\n",
    "            y_test_dis.append(+1)\n",
    "        else:\n",
    "            y_test_dis.append(-1)\n",
    "    return y_test_dis, res_sim\n",
    "\n",
    "#write the results to a file\n",
    "def writeToFile(testRes,filename):\n",
    "    resFile = open(filename, 'w')\n",
    "    for i in range (len(testRes)):\n",
    "        if testRes[i] == 1:\n",
    "            s = \"+1\"\n",
    "        else:\n",
    "            s = \"-1\"\n",
    "        resFile.write(s +'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_raw, res = readFile(\"train.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This book such life saver has been helpful able back track trends answer pediatrician questions communicate with each other when you are different times the night with newborn think one those things that everyone should required have before they leave the hospital went through all the pages the newborn version then moved the infant version and will finish the second infant book third total right our baby turns See other things that are must haves for baby '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = filterLen(docs_raw, 3)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = splitSet(docs, res, 0.2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14804"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_idf = makeVectorizer(True,(1,3),0.05,0.0008,'l2','english',lemmatizer)\n",
    "vectorizer_idf.fit(X_train)\n",
    "X_test_idf_l2_dtm = vectorizer_idf.transform(X_test)\n",
    "X_train_idf_l2_dtm =vectorizer_idf.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_sim_idf = cosSim(X_test_idf_l2_dtm, X_train_idf_l2_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_cosine_sim_idf = sortInd(cosine_sim_idf,850)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:97: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n",
      "0.194219340897\n"
     ]
    }
   ],
   "source": [
    "diff_dis_idf = 0\n",
    "mis_ind = []\n",
    "for i in range(len(y_test)):\n",
    "    diff_dis_idf += abs(int(y_test[i]) - y_test_dis_idf[i])\n",
    "    if abs(int(y_test[i]) - y_test_dis_idf[i]) >0:\n",
    "        mis_ind.append(i)\n",
    "diff_dis_idf = diff_dis_idf/2\n",
    "\n",
    "print(diff_dis_idf)\n",
    "print(diff_dis_idf/(len(y_test)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
