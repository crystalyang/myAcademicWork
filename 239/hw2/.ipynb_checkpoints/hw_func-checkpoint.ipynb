{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the source I used to do the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import string\n",
    "\n",
    "#read file\n",
    "def readFile(path):\n",
    "    with open(path,\"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        docs_raw = [l.split() for l in lines]\n",
    "        res = [int(docs_raw[i][0])for i in range(len(docs_raw))]\n",
    "    return docs_raw, res\n",
    "\n",
    "def readTest(path):\n",
    "    with open(path,\"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        docs_raw = [l.split() for l in lines]\n",
    "        #res = [int(docs_raw[i][0])for i in range(len(docs_raw))]\n",
    "    return docs_raw\n",
    "\n",
    "\n",
    "#filter doc length\n",
    "def filterLen(docs, minlen):\n",
    "    docs_raw4 = [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "    s = \" \"\n",
    "    docs = [s.join(d) for d in docs_raw4]\n",
    "    return docs\n",
    "\n",
    "# Train on a 80/20 split\n",
    "def splitSet(docs,res , testSize, stateNum):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test,y_train,y_test = train_test_split(docs,res, test_size=testSize, random_state =stateNum)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#make a lemmatizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "#convert into a matrix of token counts\n",
    "def makeVectorizer(idf,ngram, max_df,min_df,norm,stopwords,lemmatizer):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "    \n",
    "    if idf:\n",
    "        vectorizer = TfidfVectorizer(lowercase = True,\n",
    "                                    stop_words = stopwords,\n",
    "                                     use_idf = idf,\n",
    "                                     max_df = max_df,\n",
    "                                     min_df = min_df,\n",
    "                                     ngram_range = ngram,\n",
    "                                     norm = norm,\n",
    "                                     tokenizer = lemmatizer)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(lowercase = True,\n",
    "                                    stop_words = stopwords,\n",
    "                                     max_df = max_df,\n",
    "                                     min_df = min_df,\n",
    "                                    ngram_range = ngram)\n",
    "                                    \n",
    "    return vectorizer\n",
    "\n",
    "#l2 norm\n",
    "def l2Norm(data):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    normalizerl2 = Normalizer(norm = \"l2\")\n",
    "    normalizerl2.fit(data)\n",
    "    data_l2 = normalizerl2.transform(data)\n",
    "    return data_l2\n",
    "\n",
    "\n",
    "# compute the similary of the test set data to all the data in the training set\n",
    "# save the pairwise similary into a similarity matrix for later knn use\n",
    "def cosSim(test, train):\n",
    "    from sklearn.metrics.pairwise import linear_kernel\n",
    "    cosine_similarities = linear_kernel(test, train)\n",
    "    return cosine_similarities\n",
    "\n",
    "#sort the top k sim \n",
    "def sortInd(cosSim,k):\n",
    "    sortedInd = np.argpartition(cosSim, -k)[:,-k:]\n",
    "    return sortedInd\n",
    "\n",
    "#knn\n",
    "def knnDis(topInd_mat, cosSim_mat,threshold):\n",
    "    rows = topInd_mat.shape[0]\n",
    "    y_test_dis =[]\n",
    "    res_sim = []\n",
    "    for i in range(rows):\n",
    "        topInd_vec = topInd_mat[i]\n",
    "        res =sum(int(y_train[j]) * cosSim_mat[i][j] for j in topInd_vec)/sum([cosSim_mat[i][j] for j in topInd_vec])\n",
    "        \n",
    "        res_sim.append(res)\n",
    "        if res >=threshold:\n",
    "            y_test_dis.append(+1)\n",
    "        else:\n",
    "            y_test_dis.append(-1)\n",
    "    return y_test_dis, res_sim\n",
    "\n",
    "#write the results to a file\n",
    "def writeToFile(testRes,filename):\n",
    "    resFile = open(filename, 'w')\n",
    "    for i in range (len(testRes)):\n",
    "        if testRes[i] == 1:\n",
    "            s = \"+1\"\n",
    "        else:\n",
    "            s = \"-1\"\n",
    "        resFile.write(s +'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare doc data\n",
    "docs_raw, res = readFile(\"train.dat\")\n",
    "docs = filterLen(docs_raw, 3)\n",
    "docs[0]\n",
    "X_train, X_test,y_train,y_test = splitSet(docs, res, 0.2,5)\n",
    "\n",
    "#make the lemmatizer\n",
    "lemmatizer = LemmaTokenizer()\n",
    "\n",
    "\n",
    "#make csr matrix\n",
    "#vectorizer_Nonidf = makeVectorizer(False,(1,3),1.0,0.0005,'l2','english') # idf, n-gram_range, maxdf, mindf,norm, stopwords\n",
    "#vectorizer_Nonidf.fit(X_train)\n",
    "vectorizer_idf = makeVectorizer(True,(1,4),1.0,0.0005,'l2','english',lemmatizer)\n",
    "vectorizer_idf.fit(X_train)\n",
    "X_test_idf_l2_dtm = vectorizer_idf.transform(X_test)\n",
    "X_train_idf_l2_dtm =vectorizer_idf.transform(X_train)\n",
    "\n",
    "#do the norm (this is only for nonidf)\n",
    "#X_test_Nonidf_dtm_l2 = l2Norm(X_test_Nonidf_dtm)\n",
    "#X_train_Nonidf_dtm_l2 = l2Norm(X_train_Nonidf_dtm)\n",
    "\n",
    "#compute the cosine sim\n",
    "#cosine_sim_Nonidf = cosSim(X_test_Nonidf_dtm_l2, X_train_Nonidf_dtm_l2)\n",
    "cosine_sim_idf = cosSim(X_test_idf_l2_dtm, X_train_idf_l2_dtm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort and get top k sim\n",
    "#ind_cosine_sim_Nonidf= sortInd(cosine_sim_Nonidf,500)\n",
    "ind_cosine_sim_idf = sortInd(cosine_sim_idf,500)\n",
    "\n",
    "#get the results\n",
    "#y_test_dis_Nonidf,red_non = knnDis(ind_cosine_sim_Nonidf,cosine_sim_Nonidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:95: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:95: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n",
      "0.204484062669\n"
     ]
    }
   ],
   "source": [
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0.08)\n",
    "\n",
    "diff_dis_idf = 0\n",
    "mis_ind = []\n",
    "for i in range(len(y_test)):\n",
    "    diff_dis_idf += abs(int(y_test[i]) - y_test_dis_idf[i])\n",
    "    if abs(int(y_test[i]) - y_test_dis_idf[i]) >0:\n",
    "        mis_ind.append(i)\n",
    "diff_dis_idf = diff_dis_idf/2\n",
    "\n",
    "print(diff_dis_idf)\n",
    "print(diff_dis_idf/(len(y_test)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############'testRes1.dat'########### threshold 0.1 0.8381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book such life saver. has been helpful able back track trends, answer pediatrician questions, communicate with each other when you are different times the night with newborn. think one those things that everyone should required have before they leave the hospital. went through all the pages the newborn version, then moved the infant version, and will finish the second infant book (third total) right our baby turns See other things that are must haves for baby [...]\n",
      "Perfect for new parents. were able keep track baby's feeding, sleep and diaper change schedule for the first two and half months her life. Made life easier when the doctor would ask questions about habits because had all right there!\n"
     ]
    }
   ],
   "source": [
    "#prepare doc data\n",
    "train_raw, y_train= readFile(\"train.dat\")\n",
    "X_train = filterLen(train_raw, 3)\n",
    "print(X_train[0])\n",
    "\n",
    "test_raw = readTest(\"test.dat\")\n",
    "X_test = filterLen(test_raw,3)\n",
    "print(X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make the lemmatizer\n",
    "lemmatizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make csr matrix\n",
    "vectorizer_idf = makeVectorizer(True,(1,3),0.01,0.0005,'l2','english',lemmatizer)\n",
    "vectorizer_idf.fit(X_train)\n",
    "X_test_idf_l2_dtm = vectorizer_idf.transform(X_test)\n",
    "X_train_idf_l2_dtm =vectorizer_idf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the cosine sim\n",
    "cosine_sim_idf = cosSim(X_test_idf_l2_dtm, X_train_idf_l2_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort and get top k sim\n",
    "ind_cosine_sim_idf = sortInd(cosine_sim_idf,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:95: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToFile(y_test_dis_idf,'testRes1.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############'testRes2.dat'###########threshold 0 0.8295 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book such life saver. has been helpful able back track trends, answer pediatrician questions, communicate with each other when you are different times the night with newborn. think one those things that everyone should required have before they leave the hospital. went through all the pages the newborn version, then moved the infant version, and will finish the second infant book (third total) right our baby turns See other things that are must haves for baby [...]\n",
      "Perfect for new parents. were able keep track baby's feeding, sleep and diaper change schedule for the first two and half months her life. Made life easier when the doctor would ask questions about habits because had all right there!\n"
     ]
    }
   ],
   "source": [
    "#prepare doc data\n",
    "train_raw, y_train= readFile(\"train.dat\")\n",
    "X_train = filterLen(train_raw, 3)\n",
    "print(X_train[0])\n",
    "\n",
    "test_raw = readTest(\"test.dat\")\n",
    "X_test = filterLen(test_raw,3)\n",
    "print(X_test[0])\n",
    "\n",
    "\n",
    "#make the lemmatizer\n",
    "lemmatizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4cb9531437e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_test_idf_l2_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_train_idf_l2_dtm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mvectorizer_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-26751e765762>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     39\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m          \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#convert into a matrix of token counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 110\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#make csr matrix\n",
    "vectorizer_idf = makeVectorizer(True,(1,3),0.01,0.0005,'l2','english',lemmatizer)\n",
    "vectorizer_idf.fit(X_train)\n",
    "X_test_idf_l2_dtm = vectorizer_idf.transform(X_test)\n",
    "X_train_idf_l2_dtm =vectorizer_idf.transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the cosine sim\n",
    "cosine_sim_idf = cosSim(X_test_idf_l2_dtm, X_train_idf_l2_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort and get top k sim\n",
    "ind_cosine_sim_idf = sortInd(cosine_sim_idf,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:95: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writeToFile(y_test_dis_idf,'testRes2.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############'testRes3.dat'###########threshold 0.08 0.8423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book such life saver. has been helpful able back track trends, answer pediatrician questions, communicate with each other when you are different times the night with newborn. think one those things that everyone should required have before they leave the hospital. went through all the pages the newborn version, then moved the infant version, and will finish the second infant book (third total) right our baby turns See other things that are must haves for baby [...]\n",
      "Perfect for new parents. were able keep track baby's feeding, sleep and diaper change schedule for the first two and half months her life. Made life easier when the doctor would ask questions about habits because had all right there!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prepare doc data\n",
    "train_raw, y_train= readFile(\"train.dat\")\n",
    "X_train = filterLen(train_raw, 3)\n",
    "print(X_train[0])\n",
    "\n",
    "test_raw = readTest(\"test.dat\")\n",
    "X_test = filterLen(test_raw,3)\n",
    "print(X_test[0])\n",
    "\n",
    "\n",
    "#make the lemmatizer\n",
    "lemmatizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make csr matrix\n",
    "vectorizer_idf = makeVectorizer(True,(1,3),0.01,0.0005,'l2','english',lemmatizer)\n",
    "vectorizer_idf.fit(X_train)\n",
    "X_test_idf_l2_dtm = vectorizer_idf.transform(X_test)\n",
    "X_train_idf_l2_dtm =vectorizer_idf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute the cosine sim\n",
    "cosine_sim_idf = cosSim(X_test_idf_l2_dtm, X_train_idf_l2_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort and get top k sim\n",
    "ind_cosine_sim_idf = sortInd(cosine_sim_idf,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:95: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToFile(y_test_dis_idf,'testRes4.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18506"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_dis_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knnMajor(topInd_mat):\n",
    "    rows = topInd_mat.shape[0]\n",
    "    y_test_res = []\n",
    "    for i in range(rows):\n",
    "        topInd_vec = topInd_mat[i]\n",
    "        res_sum = sum([int(y_train[j]) for j in topInd_vec])\n",
    "        if res_sum >=0:\n",
    "            y_test_res.append(1)\n",
    "        else:\n",
    "            y_test_res.append(-1)\n",
    "    return y_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_res_idf = knnMajor(ind_cosine_sim_idf)\n",
    "y_test_res_Nonidf = knnMajor(ind_cosine_sim_Nonidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_idf = 0\n",
    "for i in range(len(y_test)):\n",
    "    diff_idf += abs(int(y_test[i]) - y_test_res_idf[i])\n",
    "    \n",
    "diff_idf = diff_idf/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_Nonidf = 0\n",
    "for i in range(len(y_test)):\n",
    "    diff_Nonidf += abs(int(y_test[i]) - y_test_res_Nonidf[i])\n",
    "    \n",
    "diff_Nonidf = diff_Nonidf/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knnDis(topInd_mat, cosSim_mat):\n",
    "    rows = topInd_mat.shape[0]\n",
    "    y_test_dis =[]\n",
    "    res_sim = []\n",
    "    for i in range(rows):\n",
    "        topInd_vec = topInd_mat[i]\n",
    "        res =sum(int(y_train[j]) * cosSim_mat[i][j] for j in topInd_vec)/sum([cosSim_mat[i][j] for j in topInd_vec])\n",
    "        res_sim.append(res)\n",
    "        if res >=0.12:\n",
    "            y_test_dis.append(1)\n",
    "        else:\n",
    "            y_test_dis.append(-1)\n",
    "    return y_test_dis, res_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "y_test_dis_idf,res_idf = knnDis(ind_cosine_sim_idf,cosine_sim_idf)\n",
    "y_test_dis_Nonidf,red_non = knnDis(ind_cosine_sim_Nonidf,cosine_sim_Nonidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_dis_idf = 0\n",
    "mis_ind = []\n",
    "for i in range(len(y_test)):\n",
    "    diff_dis_idf += abs(int(y_test[i]) - y_test_dis_idf[i])\n",
    "    if abs(int(y_test[i]) - y_test_dis_idf[i]) >0:\n",
    "        mis_ind.append(i)\n",
    "diff_dis_idf = diff_dis_idf/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_dis_Nonidf = 0\n",
    "for i in range(len(y_test)):\n",
    "    diff_dis_Nonidf += abs(int(y_test[i]) - y_test_dis_Nonidf[i])\n",
    "    \n",
    "diff_dis_Nonidf = diff_dis_Nonidf/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n",
      "0.220691518098\n"
     ]
    }
   ],
   "source": [
    "print(diff_dis_Nonidf)\n",
    "print(diff_dis_Nonidf/(len(y_test)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "0.190707725554\n"
     ]
    }
   ],
   "source": [
    "print(diff_dis_idf)\n",
    "print(diff_dis_idf/(len(y_test)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
